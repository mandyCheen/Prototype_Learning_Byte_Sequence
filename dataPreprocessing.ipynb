{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading x86_64 dataset from ./dataset/malware_original_x86_64.csv...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import r2pipe as r2\n",
    "from tqdm import tqdm\n",
    "\n",
    "BYTE_LENGTH = 2000\n",
    "N_GRAM_1 = 2\n",
    "N_GRAM_2 = 4\n",
    "N_WAYS = 10\n",
    "SEED = 7\n",
    "NUM_EXAMPLES = 200\n",
    "NUM_EXAMPLES_TEST = 70\n",
    "NUM_EXAMPLES_VAL = 70\n",
    "\n",
    "# load dataset\n",
    "DATASET_PATH = \"./dataset/malware_original_x86_64.csv\"\n",
    "DATASET_FOLDER = \"/home/mandy900619/data/Malware202403/\"\n",
    "CLUSTER_PATH = \"./cluster_data/\"\n",
    "CPU_ARCH = \"x86_64\"  \n",
    "EMBEDDING_PATH = \"./dataset_embedding/\" \n",
    "\n",
    "\n",
    "print(f\"Loading {CPU_ARCH} dataset from {DATASET_PATH}...\")\n",
    "dataset = pd.read_csv(DATASET_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract byte sequences from ELF files\n",
    "from elftools.elf.elffile import ELFFile\n",
    "import concurrent.futures\n",
    "\n",
    "flag = False\n",
    "\n",
    "def split_hex_string(hex_string):\n",
    "    return \" \".join([hex_string[i:i+2] for i in range(0, len(hex_string), 2)])\n",
    "\n",
    "if flag:\n",
    "    # extract byte sequences\n",
    "    print(f\"Extract byte sequences from {CPU_ARCH} dataset...\")\n",
    "    print(f\"Extracting byte sequences of length {BYTE_LENGTH}...\")\n",
    "\n",
    "    for row in tqdm(dataset.itertuples(), total=len(dataset)):\n",
    "        # open file with r2\n",
    "        byteAnalysis = r2.open(DATASET_FOLDER + row.file_name[:2] + \"/\" + row.file_name, flags=[\"-2\"])\n",
    "        out = byteAnalysis.cmd(f\"px* {BYTE_LENGTH}\")\n",
    "        byteAnalysis.cmd(\"quit\")\n",
    "        lines = out.strip().split(\"\\n\")\n",
    "        byteSeqence = [line[3:-1] for line in lines if not line.startswith(\"s-\")]\n",
    "        byteSeqence = \"\".join(byteSeqence)\n",
    "        byteSeqence = split_hex_string(byteSeqence)\n",
    "        dataset.at[row.Index, \"byte_sequence\"] = byteSeqence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output dataset\n",
    "if flag:  \n",
    "    OUTPUT_PATH = f\"./dataset/malware_original_{CPU_ARCH}_byte_sequence{BYTE_LENGTH}_split.csv\"\n",
    "    dataset.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "dataset = pd.read_csv(f\"./dataset/malware_original_{CPU_ARCH}_byte_sequence{BYTE_LENGTH}_split.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119454, 14)\n",
      "(2000, 14)\n",
      "(700, 14)\n",
      "(700, 14)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
      " 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9]\n"
     ]
    }
   ],
   "source": [
    "family = dataset['family'].value_counts()[:(N_WAYS)].index\n",
    "dataset_exp = dataset[dataset['family'].isin(family)]\n",
    "\n",
    "print(dataset_exp.shape)\n",
    "\n",
    "\n",
    "dataset_train = dataset_exp.groupby('family').sample(n=NUM_EXAMPLES, random_state=SEED)\n",
    "\n",
    "dataset_test = dataset_exp[~dataset_exp.index.isin(dataset_train.index)].groupby('family').sample(n=NUM_EXAMPLES_TEST, random_state=SEED)\n",
    "\n",
    "dataset_val = dataset_exp[~dataset_exp.index.isin(dataset_train.index) & ~dataset_exp.index.isin(dataset_test.index)].groupby('family').sample(n=NUM_EXAMPLES_VAL, random_state=SEED)\n",
    "\n",
    "print(dataset_train.shape)\n",
    "print(dataset_test.shape)\n",
    "print(dataset_val.shape)\n",
    "\n",
    "\n",
    "byteSeqenceTrain = dataset_train['byte_sequence'].values\n",
    "byteSeqenceTest = dataset_test['byte_sequence'].values\n",
    "byteSeqenceVal = dataset_val['byte_sequence'].values\n",
    "y_train = dataset_train['family'].values\n",
    "y_test = dataset_test['family'].values\n",
    "y_val = dataset_val['family'].values\n",
    "\n",
    "\n",
    "# convert y family to number\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(list(y_train) + list(y_test) + list(y_val))\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)\n",
    "y_val = le.fit_transform(y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract tf-idf features\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# 4-grams\n",
    "tfidf_vec = TfidfVectorizer(analyzer='word', ngram_range=(N_GRAM_1, N_GRAM_2), max_features=1000) # , max_features=1000\n",
    "tfidf_matrix_train = tfidf_vec.fit_transform(byteSeqenceTrain)\n",
    "tfidf_matrix_test = tfidf_vec.transform(byteSeqenceTest)\n",
    "tfidf_matrix_val = tfidf_vec.transform(byteSeqenceVal)\n",
    "\n",
    "tfidf_matrix_train = tfidf_matrix_train.toarray()\n",
    "tfidf_matrix_test = tfidf_matrix_test.toarray()\n",
    "tfidf_matrix_val = tfidf_matrix_val.toarray()\n",
    "\n",
    "label_mapping = {index: label for index, label in enumerate(le.classes_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (2000, 1000)\n",
      "Testing set shape: (700, 1000)\n",
      "Validation set shape: (700, 1000)\n",
      "Label mapping: {0: 'camelot', 1: 'gafgyt', 2: 'meterpreter', 3: 'mirai', 4: 'ngioweb', 5: 'rekoobe', 6: 'sliver', 7: 'sshdoor', 8: 'tsunami', 9: 'xmrig'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training set shape: {tfidf_matrix_train.shape}\")\n",
    "print(f\"Testing set shape: {tfidf_matrix_test.shape}\")\n",
    "print(f\"Validation set shape: {tfidf_matrix_val.shape}\")\n",
    "print(f\"Label mapping: {label_mapping}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f\"{EMBEDDING_PATH}{CPU_ARCH}_label_mapping.pickle\", 'wb') as f:\n",
    "    pickle.dump(label_mapping, f)\n",
    "    f.close()\n",
    "with open(f\"{EMBEDDING_PATH}{CPU_ARCH}_tfidf_vec_train.pickle\", 'wb') as f:\n",
    "    pickle.dump(tfidf_matrix_train, f)\n",
    "    f.close()\n",
    "with open(f\"{EMBEDDING_PATH}{CPU_ARCH}_tfidf_vec_test.pickle\", 'wb') as f:\n",
    "    pickle.dump(tfidf_matrix_test, f)\n",
    "    f.close()\n",
    "with open(f\"{EMBEDDING_PATH}{CPU_ARCH}_tfidf_vec_val.pickle\", 'wb') as f:\n",
    "    pickle.dump(tfidf_matrix_val, f)\n",
    "    f.close()\n",
    "with open(f\"{EMBEDDING_PATH}{CPU_ARCH}_y_train.pickle\", 'wb') as f:\n",
    "    pickle.dump(y_train, f)\n",
    "    f.close()\n",
    "with open(f\"{EMBEDDING_PATH}{CPU_ARCH}_y_test.pickle\", 'wb') as f:\n",
    "    pickle.dump(y_test, f)\n",
    "    f.close()\n",
    "with open(f\"{EMBEDDING_PATH}{CPU_ARCH}_y_val.pickle\", 'wb') as f:\n",
    "    pickle.dump(y_val, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format is a vector one line, each dimension value split by blank space\n",
    "# cluster preprocessing\n",
    "for key in label_mapping:\n",
    "    outputPathTrain = f\"{CLUSTER_PATH}{CPU_ARCH}_{label_mapping[key]}_train.txt\"\n",
    "    # outputPathTest = f\"{CLUSTER_PATH}{CPU_ARCH}_{label_mapping[key]}_test.txt\"\n",
    "    with open(outputPathTrain, 'w') as f:\n",
    "        for i in range(len(tfidf_matrix_train)):\n",
    "            if y_train[i] == key:\n",
    "                f.write('\\t'.join(map(str, tfidf_matrix_train[i])) + \"\\n\")\n",
    "    f.close()\n",
    "    # with open(outputPathTest, 'w') as f:\n",
    "    #     for i in range(len(tfidf_matrix_test)):\n",
    "    #         if y_test[i] == key:\n",
    "    #             f.write('\\t'.join(map(str, tfidf_matrix_test[i])) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "byteSequence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
