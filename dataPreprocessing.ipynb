{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading i386 dataset from ./dataset/malware_original_i386.csv...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import r2pipe as r2\n",
    "from tqdm import tqdm\n",
    "\n",
    "BYTE_LENGTH = 2000\n",
    "N_GRAM_1 = 2\n",
    "N_GRAM_2 = 4\n",
    "N_WAYS = 10\n",
    "SEED = 7\n",
    "NUM_EXAMPLES = 100\n",
    "NUM_EXAMPLES_TEST = 30\n",
    "NUM_EXAMPLES_VAL = 0\n",
    "\n",
    "# load dataset\n",
    "DATASET_FOLDER = \"/home/mandy900619/data/Malware202403/\"\n",
    "CLUSTER_PATH = \"./cluster_data/\"\n",
    "CPU_ARCH = \"i386\"\n",
    "DATASET_PATH = f\"./dataset/malware_original_{CPU_ARCH}.csv\"  \n",
    "EMBEDDING_PATH = \"./dataset_embedding/\" \n",
    "ADDITONAL_INFO = \"_rm_dup\"\n",
    "\n",
    "print(f\"Loading {CPU_ARCH} dataset from {DATASET_PATH}...\")\n",
    "dataset = pd.read_csv(DATASET_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "family\n",
      "xorddos        906935\n",
      "mirai           36877\n",
      "gafgyt          28643\n",
      "setag            2551\n",
      "tsunami          1999\n",
      "dofloo           1064\n",
      "ddostf            683\n",
      "meterpreter       313\n",
      "elknot            266\n",
      "chinaz            224\n",
      "kaiji             130\n",
      "dnsamp             94\n",
      "mayday             83\n",
      "race               62\n",
      "exploitscan        55\n",
      "sshbrute           55\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "family_counts = dataset['family'].value_counts()\n",
    "print(family_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract byte sequences from ELF files\n",
    "\n",
    "notHaveByteSequence = False\n",
    "removeDup = False\n",
    "\n",
    "def split_hex_string(hex_string):\n",
    "    return \" \".join([hex_string[i:i+2] for i in range(0, len(hex_string), 2)])\n",
    "\n",
    "if notHaveByteSequence:\n",
    "    # extract byte sequences\n",
    "    print(f\"Extract byte sequences from {CPU_ARCH} dataset...\")\n",
    "    print(f\"Extracting byte sequences of length {BYTE_LENGTH}...\")\n",
    "\n",
    "    for row in tqdm(dataset.itertuples(), total=len(dataset)):\n",
    "        # open file with r2\n",
    "        byteAnalysis = r2.open(DATASET_FOLDER + row.file_name[:2] + \"/\" + row.file_name, flags=[\"-2\"])\n",
    "        out = byteAnalysis.cmd(f\"px* {BYTE_LENGTH}\")\n",
    "        byteAnalysis.cmd(\"quit\")\n",
    "        lines = out.strip().split(\"\\n\")\n",
    "        byteSeqence = [line[3:-1] for line in lines if not line.startswith(\"s-\")]\n",
    "        byteSeqence = \"\".join(byteSeqence)\n",
    "        byteSeqence = split_hex_string(byteSeqence)\n",
    "        dataset.at[row.Index, \"byte_sequence\"] = byteSeqence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output dataset\n",
    "if notHaveByteSequence:  \n",
    "    OUTPUT_PATH = f\"./dataset/malware_original_{CPU_ARCH}_byte_sequence{BYTE_LENGTH}_split.csv\"\n",
    "    dataset.to_csv(OUTPUT_PATH, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if removeDup:\n",
    "    dataset = pd.read_csv(f\"./dataset/malware_original_{CPU_ARCH}_byte_sequence{BYTE_LENGTH}_split.csv\")\n",
    "    # remove duplicate rows based on byte_sequence\n",
    "    print(\"Original dataset shape:\", dataset.shape)\n",
    "    print(\"Removing duplicate rows based on byte_sequence...\")\n",
    "    dataset_rm_dup = dataset.drop_duplicates(subset=\"byte_sequence\")\n",
    "    dataset_rm_dup = dataset_rm_dup.reset_index(drop=True)\n",
    "    print(\"Dataset shape after removing duplicates:\", dataset_rm_dup.shape)\n",
    "    family_counts = dataset_rm_dup[\"family\"].value_counts()\n",
    "    print(family_counts[:])\n",
    "\n",
    "    # output dataset\n",
    "    OUTPUT_PATH = f\"./dataset/malware_original_{CPU_ARCH}_byte_sequence{BYTE_LENGTH}_split{ADDITONAL_INFO}.csv\"\n",
    "    dataset_rm_dup.to_csv(OUTPUT_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "family\n",
      "mirai           14098\n",
      "gafgyt           7927\n",
      "tsunami           679\n",
      "race               51\n",
      "exploitscan        47\n",
      "xorddos            41\n",
      "sshdoor            33\n",
      "local              32\n",
      "rkit               31\n",
      "sshbrute           30\n",
      "kaiji              28\n",
      "sliver             27\n",
      "backegmm           27\n",
      "dnsamp             27\n",
      "pnscan             26\n",
      "meterpreter        26\n",
      "prochider          26\n",
      "cornelgen          24\n",
      "equationdrug       21\n",
      "sckit              20\n",
      "chinaz             17\n",
      "ddostf             15\n",
      "setag              15\n",
      "elknot             12\n",
      "dofloo              7\n",
      "mayday              6\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(f\"./dataset/malware_original_{CPU_ARCH}_byte_sequence{BYTE_LENGTH}_split{ADDITONAL_INFO}.csv\")\n",
    "family_counts = dataset[\"family\"].value_counts()\n",
    "print(family_counts[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33994, 14)\n",
      "(1000, 14)\n",
      "(300, 14)\n"
     ]
    }
   ],
   "source": [
    "family = dataset['family'].value_counts()[:(N_WAYS)].index\n",
    "dataset_exp = dataset[dataset['family'].isin(family)]\n",
    "\n",
    "print(dataset_exp.shape)\n",
    "\n",
    "\n",
    "dataset_train = dataset_exp.groupby('family').sample(n=NUM_EXAMPLES, random_state=SEED)\n",
    "\n",
    "dataset_test = dataset_exp[~dataset_exp.index.isin(dataset_train.index)].groupby('family').sample(n=NUM_EXAMPLES_TEST, random_state=SEED)\n",
    "if NUM_EXAMPLES_VAL > 0:\n",
    "    dataset_val = dataset_exp[~dataset_exp.index.isin(dataset_train.index) & ~dataset_exp.index.isin(dataset_test.index)].groupby('family').sample(n=NUM_EXAMPLES_VAL, random_state=SEED)\n",
    "\n",
    "print(dataset_train.shape)\n",
    "print(dataset_test.shape)\n",
    "if NUM_EXAMPLES_VAL > 0:\n",
    "    print(dataset_val.shape)\n",
    "\n",
    "\n",
    "byteSeqenceTrain = dataset_train['byte_sequence'].values\n",
    "byteSeqenceTest = dataset_test['byte_sequence'].values\n",
    "if NUM_EXAMPLES_VAL > 0:\n",
    "    byteSeqenceVal = dataset_val['byte_sequence'].values\n",
    "y_train = dataset_train['family'].values\n",
    "y_test = dataset_test['family'].values\n",
    "if NUM_EXAMPLES_VAL > 0:\n",
    "    y_val = dataset_val['family'].values\n",
    "\n",
    "\n",
    "# convert y family to number\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "if NUM_EXAMPLES_VAL > 0:\n",
    "    list_ = list(y_train) + list(y_test) + list(y_val)\n",
    "else:\n",
    "    list_ = list(y_train) + list(y_test)\n",
    "le.fit(list_)\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)\n",
    "if NUM_EXAMPLES_VAL > 0:\n",
    "    y_val = le.fit_transform(y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract tf-idf features\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# 4-grams\n",
    "tfidf_vec = TfidfVectorizer(analyzer='word', ngram_range=(N_GRAM_1, N_GRAM_2), max_features=1000) # , max_features=1000\n",
    "tfidf_matrix_train = tfidf_vec.fit_transform(byteSeqenceTrain)\n",
    "tfidf_matrix_test = tfidf_vec.transform(byteSeqenceTest)\n",
    "if NUM_EXAMPLES_VAL > 0:\n",
    "    tfidf_matrix_val = tfidf_vec.transform(byteSeqenceVal)\n",
    "\n",
    "tfidf_matrix_train = tfidf_matrix_train.toarray()\n",
    "tfidf_matrix_test = tfidf_matrix_test.toarray()\n",
    "if NUM_EXAMPLES_VAL > 0:\n",
    "    tfidf_matrix_val = tfidf_matrix_val.toarray()\n",
    "\n",
    "label_mapping = {index: label for index, label in enumerate(le.classes_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (1000, 1000)\n",
      "Testing set shape: (300, 1000)\n",
      "Label mapping: {0: 'camelot', 1: 'dropperl', 2: 'gafgyt', 3: 'mirai', 4: 'rekoobe', 5: 'sliver', 6: 'sshdoor', 7: 'tsunami', 8: 'vtflooder', 9: 'xmrig'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training set shape: {tfidf_matrix_train.shape}\")\n",
    "print(f\"Testing set shape: {tfidf_matrix_test.shape}\")\n",
    "if NUM_EXAMPLES_VAL > 0:\n",
    "    print(f\"Validation set shape: {tfidf_matrix_val.shape}\")\n",
    "print(f\"Label mapping: {label_mapping}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"{EMBEDDING_PATH}{CPU_ARCH}_label_mapping{ADDITONAL_INFO}.pickle\", 'wb') as f:\n",
    "    pickle.dump(label_mapping, f)\n",
    "    f.close()\n",
    "with open(f\"{EMBEDDING_PATH}{CPU_ARCH}_tfidf_vec_train{ADDITONAL_INFO}.pickle\", 'wb') as f:\n",
    "    pickle.dump(tfidf_matrix_train, f)\n",
    "    f.close()\n",
    "with open(f\"{EMBEDDING_PATH}{CPU_ARCH}_tfidf_vec_test{ADDITONAL_INFO}.pickle\", 'wb') as f:\n",
    "    pickle.dump(tfidf_matrix_test, f)\n",
    "    f.close()\n",
    "if NUM_EXAMPLES_VAL > 0:\n",
    "    with open(f\"{EMBEDDING_PATH}{CPU_ARCH}_tfidf_vec_val{ADDITONAL_INFO}.pickle\", 'wb') as f:\n",
    "        pickle.dump(tfidf_matrix_val, f)\n",
    "        f.close()\n",
    "with open(f\"{EMBEDDING_PATH}{CPU_ARCH}_y_train{ADDITONAL_INFO}.pickle\", 'wb') as f:\n",
    "    pickle.dump(y_train, f)\n",
    "    f.close()\n",
    "with open(f\"{EMBEDDING_PATH}{CPU_ARCH}_y_test{ADDITONAL_INFO}.pickle\", 'wb') as f:\n",
    "    pickle.dump(y_test, f)\n",
    "    f.close()\n",
    "if NUM_EXAMPLES_VAL > 0:\n",
    "    with open(f\"{EMBEDDING_PATH}{CPU_ARCH}_y_val{ADDITONAL_INFO}.pickle\", 'wb') as f:\n",
    "        pickle.dump(y_val, f)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format is a vector one line, each dimension value split by blank space\n",
    "# cluster preprocessing\n",
    "for key in label_mapping:\n",
    "    outputPathTrain = f\"{CLUSTER_PATH}{CPU_ARCH}_{label_mapping[key]}_train{ADDITONAL_INFO}.txt\"\n",
    "    # outputPathTest = f\"{CLUSTER_PATH}{CPU_ARCH}_{label_mapping[key]}_test.txt\"\n",
    "    with open(outputPathTrain, 'w') as f:\n",
    "        for i in range(len(tfidf_matrix_train)):\n",
    "            if y_train[i] == key:\n",
    "                f.write('\\t'.join(map(str, tfidf_matrix_train[i])) + \"\\n\")\n",
    "    f.close()\n",
    "    # with open(outputPathTest, 'w') as f:\n",
    "    #     for i in range(len(tfidf_matrix_test)):\n",
    "    #         if y_test[i] == key:\n",
    "    #             f.write('\\t'.join(map(str, tfidf_matrix_test[i])) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "byteSequence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
